{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SloughGPT Training on Google Colab\n",
        "\n",
        "Train your own SloughGPT model with GPU acceleration!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/iamtowbee/sloughGPT.git\n",
        "%cd sloughGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "datasets_dir = Path(\"datasets\")\n",
        "datasets_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# CHOOSE YOUR DATASET - Enter path or URL below\n",
        "# Optionsn",
        "#:\ 1. \"shakespeare\" - Download Shakespeare dataset\n",
        "# 2. \"tiny\" - Download TinyStories dataset (smaller)\n",
        "# 3. \"/path/to/your/file.txt\" - Use your own file\n",
        "# 4. \"https://example.com/data.txt\" - Download from URL\n",
        "\n",
        "DATASET_CHOICE = \"shakespeare\"  # <-- CHANGE THIS!\n",
        "\n",
        "def get_dataset(choice):\n",
        "    \"\"\"Get dataset based on choice.\"\"\"\n",
        "    if choice == \"shakespeare\":\n",
        "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "        path = datasets_dir / \"shakespeare.txt\"\n",
        "        if not path.exists():\n",
        "            print(\"Downloading Shakespeare dataset...\")\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "            print(f\"Downloaded to: {path}\")\n",
        "        return path\n",
        "    \n",
        "    elif choice == \"tiny\":\n",
        "        url = \"https://raw.githubusercontent.com/dariush-bahrami/TinyStories/main/TinyStories-data.txt\"\n",
        "        path = datasets_dir / \"tiny_stories.txt\"\n",
        "        if not path.exists():\n",
        "            print(\"Downloading TinyStories dataset...\")\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "            print(f\"Downloaded to: {path}\")\n",
        "        return path\n",
        "    \n",
        "    elif choice.startswith(\"http\"):\n",
        "        # Download from URL\n",
        "        filename = choice.split(\"/\")[-1]\n",
        "        path = datasets_dir / filename\n",
        "        if not path.exists():\n",
        "            print(f\"Downloading {choice}...\")\n",
        "            urllib.request.urlretrieve(choice, path)\n",
        "        return path\n",
        "    \n",
        "    else:\n",
        "        # Assume it's a file path\n",
        "        path = Path(choice)\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"Dataset not found: {path}\")\n",
        "        return path\n",
        "\n",
        "# Get dataset path\n",
        "try:\n",
        "    data_path = get_dataset(DATASET_CHOICE)\n",
        "    \n",
        "    # Verify dataset\n",
        "    with open(data_path) as f:\n",
        "        text = f.read()\n",
        "    \n",
        "    print(f\"âœ… Dataset loaded: {data_path}\")\n",
        "    print(f\"   Size: {len(text):,} characters\")\n",
        "    print(f\"   Preview: {text[:100].replace(chr(10), 'â†µ')}...\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"âŒ ERROR: {e}\")\n",
        "    print(\"\\nðŸ’¡ Available options:\")\n",
        "    print(\"   - 'shakespeare' - Shakespeare's works\")\n",
        "    print(\"   - 'tiny' - TinyStories dataset\")\n",
        "    print(\"   - '/path/to/your/file.txt' - Your local file\")\n",
        "    print(\"   - 'https://example.com/data.txt' - Download from URL\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ERROR loading dataset: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "from domains.training import TrainingConfig, Trainer\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ðŸ–¥ï¸  Training on: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# âš™ï¸ CONFIGURATION - Adjust these values!\n",
        "CONFIG = {\n",
        "    \"epochs\": 5,         # Number of training epochs\n",
        "    \"batch_size\": 64,     # Batch size\n",
        "    \"n_embed\": 256,        # Embedding dimension\n",
        "    \"n_layer\": 6,          # Number of transformer layers\n",
        "    \"n_head\": 8,          # Number of attention heads\n",
        "    \"vocab_size\": 5000,   # Vocabulary size\n",
        "    \"learning_rate\": 1e-3, # Learning rate\n",
        "}\n",
        "\n",
        "print(\"âš™ï¸  Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"   {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "# Create config\n",
        "config = TrainingConfig(\n",
        "    data_path=str(data_path),\n",
        "    epochs=CONFIG[\"epochs\"],\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    n_embed=CONFIG[\"n_embed\"],\n",
        "    n_layer=CONFIG[\"n_layer\"],\n",
        "    vocab_size=CONFIG[\"vocab_size\"],\n",
        "    learning_rate=CONFIG[\"learning_rate\"],\n",
        "    max_batches=300,  # Batches per epoch\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "print(\"ðŸ”§ Creating trainer...\")\n",
        "trainer = Trainer(config)\n",
        "trainer.setup()\n",
        "\n",
        "num_params = sum(p.numel() for p in trainer.model.model.parameters())\n",
        "print(f\"ðŸ“Š Model parameters: {num_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop with progress bar\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "ðŸš€ STARTING TRAINING\n",
        "=\"*60)\n",
        "\n",
        "trainer.model.model.train()\n",
        "optimizer = torch.optim.AdamW(trainer.model.model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
        "\n",
        "total_batches = CONFIG[\"epochs\"] * config.max_batches\n",
        "\n",
        "# Progress bar\n",
        "pbar = tqdm(total=total_batches, desc=\"Training\", unit=\"batch\")\n",
        "\n",
        "epoch_losses = []\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(CONFIG[\"epochs\"]):\n",
        "    epoch_loss = 0\n",
        "    batch_count = 0\n",
        "    \n",
        "    batch_gen = trainer.data_loader.get_batch(CONFIG[\"batch_size\"], 128)\n",
        "    \n",
        "    for batch_idx in range(config.max_batches):\n",
        "        try:\n",
        "            x, y = next(batch_gen)\n",
        "        except StopIteration:\n",
        "            break\n",
        "        \n",
        "        x_t = torch.tensor(x.astype(np.int64), dtype=torch.long).to(device)\n",
        "        y_t = torch.tensor(y.astype(np.int64), dtype=torch.long).to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        logits, loss = trainer.model.model(x_t, y_t)\n",
        "        \n",
        "        if loss is not None:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(trainer.model.model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            batch_count += 1\n",
        "        \n",
        "        pbar.update(1)\n",
        "        pbar.set_postfix({\n",
        "            \"loss\": f\"{loss.item() if loss else 0:.4f}\",\n",
        "            \"epoch\": epoch+1\n",
        "        })\n",
        "    \n",
        "    avg_loss = epoch_loss / max(batch_count, 1)\n",
        "    epoch_losses.append(avg_loss)\n",
        "    \n",
        "    # Time estimate\n",
        "    elapsed = time.time() - start_time\n",
        "    avg_time = elapsed / (epoch + 1)\n",
        "    remaining = avg_time * (CONFIG[\"epochs\"] - epoch - 1)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{CONFIG['epochs']} | Loss: {avg_loss:.4f} | ETA: {remaining/60:.1f}min\")\n",
        "    \n",
        "    pbar.refresh()\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"âœ… TRAINING COMPLETE in {total_time/60:.1f} minutes!\")\n",
        "print(f\"   Final loss: {epoch_losses[-1]:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Save trained model\n",
        "output_dir = Path(\"models/sloughgpt\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model_path = output_dir / \"sloughgpt_trained.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': trainer.model.model.state_dict(),\n",
        "    'config': CONFIG,\n",
        "    'data_path': str(data_path),\n",
        "}, model_path)\n",
        "\n",
        "print(f\"ðŸ’¾ Model saved to: {model_path}\")\n",
        "\n",
        "# Download to local machine\n",
        "from google.colab import files\n",
        "files.download(str(model_path))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
