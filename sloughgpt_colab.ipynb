{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SloughGPT Training on Google Colab\n",
        "\n",
        "Train your own SloughGPT model with GPU acceleration!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/iamtowbee/sloughGPT.git\n",
        "%cd sloughGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "datasets_dir = Path(\"datasets\")\n",
        "datasets_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# CHOOSE YOUR DATASET - Enter path or URL below\n",
        "# Options\n",
        "# 1. \"shakespeare\" - Download Shakespeare dataset\n",
        "# 2. \"tiny\" - Download TinyStories dataset (smaller)\n",
        "# 3. \"/path/to/your/file.txt\" - Use your own file\n",
        "# 4. \"https://example.com/data.txt\" - Download from URL\n",
        "\n",
        "DATASET_CHOICE = \"shakespeare\"  # <-- CHANGE THIS!\n",
        "\n",
        "def get_dataset(choice):\n",
        "    \"\"\"Get dataset based on choice.\"\"\"\n",
        "    if choice == \"shakespeare\":\n",
        "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "        path = datasets_dir / \"shakespeare.txt\"\n",
        "        if not path.exists():\n",
        "            print(\"Downloading Shakespeare dataset...\")\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "            print(f\"Downloaded to: {path}\")\n",
        "        return path\n",
        "    \n",
        "    elif choice == \"tiny\":\n",
        "        url = \"https://raw.githubusercontent.com/dariush-bahrami/TinyStories/main/TinyStories-data.txt\"\n",
        "        path = datasets_dir / \"tiny_stories.txt\"\n",
        "        if not path.exists():\n",
        "            print(\"Downloading TinyStories dataset...\")\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "            print(f\"Downloaded to: {path}\")\n",
        "        return path\n",
        "    \n",
        "    elif choice.startswith(\"http\"):\n",
        "        # Download from URL\n",
        "        filename = choice.split(\"/\")[-1]\n",
        "        path = datasets_dir / filename\n",
        "        if not path.exists():\n",
        "            print(f\"Downloading {choice}...\")\n",
        "            urllib.request.urlretrieve(choice, path)\n",
        "        return path\n",
        "    \n",
        "    else:\n",
        "        # Assume it's a file path\n",
        "        path = Path(choice)\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"Dataset not found: {path}\")\n",
        "        return path\n",
        "\n",
        "# Get dataset path\n",
        "try:\n",
        "    data_path = get_dataset(DATASET_CHOICE)\n",
        "    \n",
        "    # Verify dataset\n",
        "    with open(data_path) as f:\n",
        "        text = f.read()\n",
        "    \n",
        "    print(f\"‚úÖ Dataset loaded: {data_path}\")\n",
        "    print(f\"   Size: {len(text):,} characters\")\n",
        "    print(f\"   Preview: {text[:100].replace(chr(10), '‚Üµ')}...\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå ERROR: {e}\")\n",
        "    print(\"\\nüí° Available options:\")\n",
        "    print(\"   - 'shakespeare' - Shakespeare's works\")\n",
        "    print(\"   - 'tiny' - TinyStories dataset\")\n",
        "    print(\"   - '/path/to/your/file.txt' - Your local file\")\n",
        "    print(\"   - 'https://example.com/data.txt' - Download from URL\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR loading dataset: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "from domains.training import TrainingConfig, Trainer\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üñ•Ô∏è  Training on: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è CONFIGURATION - Adjust these values!\n",
        "CONFIG = {\n",
        "    \"epochs\": 5,         # Number of training epochs\n",
        "    \"batch_size\": 64,     # Batch size\n",
        "    \"n_embed\": 256,        # Embedding dimension\n",
        "    \"n_layer\": 6,          # Number of transformer layers\n",
        "    \"n_head\": 8,          # Number of attention heads\n",
        "    \"vocab_size\": 5000,   # Vocabulary size\n",
        "    \"learning_rate\": 1e-3, # Learning rate\n",
        "    \n",
        "    # CPU Optimization Settings\n",
        "    \"gradient_accumulation_steps\": 1,  # Accumulate gradients (effective batch = batch_size * grad_accum)\n",
        "    \"use_mixed_precision\": torch.cuda.is_available(),  # Use FP16 on GPU\n",
        "    \"num_workers\": 2,      # DataLoader workers (CPU only)\n",
        "    \"gradient_clip\": 1.0,   # Gradient clipping\n",
        "    \"weight_decay\": 0.01,  # AdamW weight decay\n",
        "    \"warmup_epochs\": 1,   # Learning rate warmup\n",
        "}\n",
        "\n",
        "print(\"‚öôÔ∏è  Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "\n",
        "# CPU-specific optimizations\n",
        "if device == \"cpu\":\n",
        "    print(\"\\nüîß CPU Optimizations Enabled:\")\n",
        "    CONFIG[\"use_mixed_precision\"] = False  # No FP16 on CPU\n",
        "    CONFIG[\"num_workers\"] = min(CONFIG[\"num_workers\"], 4)\n",
        "    print(f\"   - DataLoader workers: {CONFIG['num_workers']}\")\n",
        "    print(f\"   - Gradient accumulation: {CONFIG['gradient_accumulation_steps']}\")\n",
        "    print(f\"   - Gradient clipping: {CONFIG['gradient_clip']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "# Create config\n",
        "config = TrainingConfig(\n",
        "    data_path=str(data_path),\n",
        "    epochs=CONFIG[\"epochs\"],\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    n_embed=CONFIG[\"n_embed\"],\n",
        "    n_layer=CONFIG[\"n_layer\"],\n",
        "    vocab_size=CONFIG[\"vocab_size\"],\n",
        "    learning_rate=CONFIG[\"learning_rate\"],\n",
        "    max_batches=300,  # Batches per epoch\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "print(\"üîß Creating trainer...\")\n",
        "trainer = Trainer(config)\n",
        "trainer.setup()\n",
        "\n",
        "num_params = sum(p.numel() for p in trainer.model.model.parameters())\n",
        "print(f\"üìä Model parameters: {num_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop with progress bar and optimizations\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "üöÄ STARTING TRAINING\n",
        "=\"*60)\n",
        "\n",
        "trainer.model.model.train()\n",
        "\n",
        "# Optimizer with weight decay\n",
        "optimizer = torch.optim.AdamW(\n",
        "    trainer.model.model.parameters(), \n",
        "    lr=CONFIG[\"learning_rate\"],\n",
        "    weight_decay=CONFIG.get(\"weight_decay\", 0.01)\n",
        ")\n",
        "\n",
        "# Learning rate scheduler with warmup\n",
        "def lr_lambda(step):\n",
        "    warmup_steps = CONFIG.get(\"warmup_epochs\", 1) * config.max_batches\n",
        "    if step < warmup_steps:\n",
        "        return step / max(1, warmup_steps)\n",
        "    else:\n",
        "        return max(0.01, 1 - (step - warmup_steps) / (CONFIG[\"epochs\"] * config.max_batches - warmup_steps))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "total_batches = CONFIG[\"epochs\"] * config.max_batches\n",
        "grad_accum = CONFIG.get(\"gradient_accumulation_steps\", 1)\n",
        "\n",
        "# Progress bar\n",
        "pbar = tqdm(total=total_batches, desc=\"Training\", unit=\"batch\")\n",
        "\n",
        "epoch_losses = []\n",
        "start_time = time.time()\n",
        "accumulated_loss = 0\n",
        "\n",
        "for epoch in range(CONFIG[\"epochs\"]):\n",
        "    epoch_loss = 0\n",
        "    batch_count = 0\n",
        "    \n",
        "    batch_gen = trainer.data_loader.get_batch(CONFIG[\"batch_size\"], 128)\n",
        "    \n",
        "    for batch_idx in range(config.max_batches):\n",
        "        try:\n",
        "            x, y = next(batch_gen)\n",
        "        except StopIteration:\n",
        "            break\n",
        "        \n",
        "        x_t = torch.tensor(x.astype(np.int64), dtype=torch.long).to(device)\n",
        "        y_t = torch.tensor(y.astype(np.int64), dtype=torch.long).to(device)\n",
        "        \n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits, loss = trainer.model.model(x_t, y_t)\n",
        "        \n",
        "        if loss is not None:\n",
        "            # Scale loss for gradient accumulation\n",
        "            loss = loss / grad_accum\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient accumulation check\n",
        "            if (batch_idx + 1) % grad_accum == 0 or (batch_idx + 1) == config.max_batches:\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    trainer.model.model.parameters(), \n",
        "                    CONFIG.get(\"gradient_clip\", 1.0)\n",
        "                )\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "            \n",
        "            accumulated_loss += loss.item() * grad_accum\n",
        "            epoch_loss += accumulated_loss\n",
        "            batch_count += 1\n",
        "            accumulated_loss = 0\n",
        "        \n",
        "        pbar.update(1)\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        pbar.set_postfix({\n",
        "            \"loss\": f\"{(loss.item() * grad_accum if loss else 0):.4f}\",\n",
        "            \"epoch\": epoch+1,\n",
        "            \"lr\": f\"{current_lr:.6f}\"\n",
        "        })\n",
        "    \n",
        "    avg_loss = epoch_loss / max(batch_count, 1)\n",
        "    epoch_losses.append(avg_loss)\n",
        "    \n",
        "    # Time estimate\n",
        "    elapsed = time.time() - start_time\n",
        "    batches_done = (epoch + 1) * config.max_batches\n",
        "    eta = (elapsed / batches_done) * (total_batches - batches_done) if batches_done > 0 else 0\n",
        "    \n",
        "    print(f\"\\nüìä Epoch {epoch+1}/{CONFIG['epochs']} | Loss: {avg_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f} | ETA: {eta/60:.1f}min\")\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "print(f\"   Total time: {(time.time()-start_time)/60:.1f} minutes\")\n",
        "print(f\"   Final loss: {epoch_losses[-1]:.4f}\")"
      ]
    },\n",
        "    avg_time = elapsed / (epoch + 1)\n",
        "    remaining = avg_time * (CONFIG[\"epochs\"] - epoch - 1)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{CONFIG['epochs']} | Loss: {avg_loss:.4f} | ETA: {remaining/60:.1f}min\")\n",
        "    \n",
        "    pbar.refresh()\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"‚úÖ TRAINING COMPLETE in {total_time/60:.1f} minutes!\")\n",
        "print(f\"   Final loss: {epoch_losses[-1]:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Save trained model\n",
        "output_dir = Path(\"models/sloughgpt\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model_path = output_dir / \"sloughgpt_trained.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': trainer.model.model.state_dict(),\n",
        "    'config': CONFIG,\n",
        "    'data_path': str(data_path),\n",
        "}, model_path)\n",
        "\n",
        "print(f\"üíæ Model saved to: {model_path}\")\n",
        "\n",
        "# Download to local machine\n",
        "from google.colab import files\n",
        "files.download(str(model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text with trained model\n",
        "print(\"=\"*60)\n",
        "üéØ TEXT GENERATION\n",
        "=\"*60)\n",
        "\n",
        "# Your prompt - change this!\n",
        "PROMPT = \"The king\"\n",
        "MAX_TOKENS = 200\n",
        "TEMPERATURE = 0.8\n",
        "\n",
        "print(f\"Prompt: '{PROMPT}'\")\n",
        "print(f\"Generating {MAX_TOKENS} tokens...\")\n",
        "\n",
        "# Set model to eval mode\n",
        "trainer.model.model.eval()\n",
        "\n",
        "# Encode prompt\n",
        "import numpy as np\n",
        "tokenizer = trainer.data_loader.tokenizer\n",
        "\n",
        "# Simple generation\n",
        "if hasattr(tokenizer, 'encode'):\n",
        "    idx = tokenizer.encode(PROMPT)\n",
        "else:\n",
        "    chars = sorted(set(trainer.data_loader.texts[0]))\n",
        "    stoi = {c: i for i, c in enumerate(chars)}\n",
        "    idx = [stoi.get(c, 0) for c in PROMPT]\n",
        "\n",
        "idx = torch.tensor([idx], dtype=torch.long).to(device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    for _ in range(MAX_TOKENS):\n",
        "        idx_cond = idx[:, -128:]\n",
        "        \n",
        "        logits, _ = trainer.model.model(idx_cond)\n",
        "        logits = logits[:, -1, :] / TEMPERATURE\n",
        "        \n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, idx_next], dim=1)\n",
        "\n",
        "# Decode\n",
        "if hasattr(tokenizer, 'decode'):\n",
        "    generated = tokenizer.decode(idx[0].tolist())\n",
        "else:\n",
        "    chars = sorted(set(trainer.data_loader.texts[0]))\n",
        "    itos = {i: c for i, c in enumerate(chars)}\n",
        "    generated = ''.join([itos.get(i, '') for i in idx[0].tolist()])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "üìù GENERATED TEXT:\n",
        "=\"*60)\n",
        "print(generated)\n",
        "=\"*60"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
