{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SloughGPT - Enterprise AI Framework\n",
        "\n",
        "Train production-ready AI models with industry-standard features:\n",
        "‚úÖ LR Schedulers (Cosine, Warmup, OneCycle)\n",
        "‚úÖ Mixed Precision (FP16/BF16)\n",
        "‚úÖ Gradient Accumulation\n",
        "‚úÖ Personality Training\n",
        "‚úÖ .sou Model Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/iamtowbee/sloughGPT.git\n",
        "%cd sloughGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "datasets_dir = Path(\"datasets\")\n",
        "datasets_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# CHOOSE YOUR DATASET\n",
        "DATASET_CHOICE = \"shakespeare\"  # Options: \"shakespeare\", \"tiny\", or \"/path/to/file.txt\"\n",
        "\n",
        "def get_dataset(choice):\n",
        "    if choice == \"shakespeare\":\n",
        "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "        path = datasets_dir / \"shakespeare.txt\"\n",
        "        if not path.exists():\n",
        "            print(\"Downloading Shakespeare dataset...\")\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "        return path\n",
        "    elif choice == \"tiny\":\n",
        "        url = \"https://raw.githubusercontent.com/dariush-bahrami/TinyStories/main/TinyStories-data.txt\"\n",
        "        path = datasets_dir / \"tiny_stories.txt\"\n",
        "        if not path.exists():\n",
        "            print(\"Downloading TinyStories dataset...\")\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "        return path\n",
        "    else:\n",
        "        path = Path(choice)\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"Dataset not found: {path}\")\n",
        "        return path\n",
        "\n",
        "data_path = get_dataset(DATASET_CHOICE)\n",
        "with open(data_path) as f:\n",
        "    text = f.read()\n",
        "print(f\"‚úÖ Dataset: {data_path.name}, {len(text):,} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Check GPU & Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üñ•Ô∏è  Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Import SloughGPT modules\n",
        "from domains.training import TrainingConfig, Trainer\n",
        "from domains.training.lr_schedulers import create_scheduler\n",
        "print(\"‚úÖ SloughGPT modules imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training Configuration (Industry Standard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è PRODUCTION TRAINING CONFIGURATION\n",
        "CONFIG = {\n",
        "    # Model\n",
        "    \"model_id\": \"nanogpt-nanogpt\",\n",
        "    \"n_embed\": 256,\n",
        "    \"n_layer\": 6,\n",
        "    \"n_head\": 8,\n",
        "    \"vocab_size\": 5000,\n",
        "    \n",
        "    # Training\n",
        "    \"epochs\": 5,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"max_batches\": 300,\n",
        "    \n",
        "    # üéØ LR Scheduler (NEW!)\n",
        "    \"scheduler\": \"cosine\",        # none, cosine, warmup, onecycle, cyclic, polynomial\n",
        "    \"warmup_steps\": 100,         # Linear warmup steps\n",
        "    \"min_lr\": 1e-6,              # Minimum learning rate\n",
        "    \"max_lr\": 1e-3,             # Maximum learning rate (for onecycle/cyclic)\n",
        "    \n",
        "    # üéØ Mixed Precision (NEW!)\n",
        "    \"precision\": \"bf16\" if device == \"cuda\" else \"fp32\",  # fp32, fp16, bf16\n",
        "    \n",
        "    # üéØ Gradient Accumulation (NEW!)\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "}\n",
        "\n",
        "print(\"üìã Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"   {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create TrainingConfig with new features\n",
        "config = TrainingConfig(\n",
        "    data_path=str(data_path),\n",
        "    model_id=CONFIG[\"model_id\"],\n",
        "    n_embed=CONFIG[\"n_embed\"],\n",
        "    n_layer=CONFIG[\"n_layer\"],\n",
        "    n_head=CONFIG[\"n_head\"],\n",
        "    vocab_size=CONFIG[\"vocab_size\"],\n",
        "    \n",
        "    epochs=CONFIG[\"epochs\"],\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    learning_rate=CONFIG[\"learning_rate\"],\n",
        "    max_batches=CONFIG[\"max_batches\"],\n",
        "    \n",
        "    # New features\n",
        "    scheduler=CONFIG[\"scheduler\"],\n",
        "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
        "    min_lr=CONFIG[\"min_lr\"],\n",
        "    max_lr=CONFIG[\"max_lr\"],\n",
        "    precision=CONFIG[\"precision\"],\n",
        "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
        "    max_grad_norm=CONFIG[\"max_grad_norm\"],\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "print(\"üîß Initializing trainer...\")\n",
        "trainer = Trainer(config)\n",
        "trainer.setup()\n",
        "\n",
        "num_params = sum(p.numel() for p in trainer.model.model.parameters())\n",
        "print(f\"üìä Model parameters: {num_params:,}\")\n",
        "print(f\"   Scheduler: {trainer.scheduler.__class__.__name__ if trainer.scheduler else 'None'}\")\n",
        "print(f\"   Precision: {CONFIG['precision']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Model (with LR Scheduler & Mixed Precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ STARTING TRAINING (with LR Scheduler & Mixed Precision)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer.model.model.train()\n",
        "\n",
        "total_batches = CONFIG[\"epochs\"] * CONFIG[\"max_batches\"]\n",
        "grad_accum = CONFIG[\"gradient_accumulation_steps\"]\n",
        "\n",
        "pbar = tqdm(total=total_batches, desc=\"Training\", unit=\"batch\")\n",
        "\n",
        "epoch_losses = []\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(CONFIG[\"epochs\"]):\n",
        "    epoch_loss = 0\n",
        "    batch_count = 0\n",
        "    \n",
        "    batch_gen = trainer.data_loader.get_batch(CONFIG[\"batch_size\"], 128)\n",
        "    \n",
        "    for batch_idx in range(config.max_batches):\n",
        "        try:\n",
        "            x, y = next(batch_gen)\n",
        "        except StopIteration:\n",
        "            break\n",
        "        \n",
        "        x_t = torch.tensor(x.astype(np.int64), dtype=torch.long).to(device)\n",
        "        y_t = torch.tensor(y.astype(np.int64), dtype=torch.long).to(device)\n",
        "        \n",
        "        trainer.optimizer.zero_grad(set_to_none=True)\n",
        "        \n",
        "        # Determine dtype for mixed precision\n",
        "        if config.precision in (\"fp16\", \"mixed\"):\n",
        "            amp_dtype = torch.float16\n",
        "        elif config.precision == \"bf16\":\n",
        "            amp_dtype = torch.bfloat16\n",
        "        else:\n",
        "            amp_dtype = None\n",
        "        \n",
        "        # Forward with mixed precision\n",
        "        if amp_dtype and trainer.scaler is not None:\n",
        "            with torch.autocast(device_type=device, dtype=amp_dtype):\n",
        "                logits, loss = trainer.model.model(x_t, y_t)\n",
        "            \n",
        "            if loss is not None:\n",
        "                loss = loss / grad_accum\n",
        "                trainer.scaler.scale(loss).backward()\n",
        "                \n",
        "                if (batch_idx + 1) % grad_accum == 0:\n",
        "                    trainer.scaler.unscale_(trainer.optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(\n",
        "                        trainer.model.model.parameters(), \n",
        "                        CONFIG[\"max_grad_norm\"]\n",
        "                    )\n",
        "                    trainer.scaler.step(trainer.optimizer)\n",
        "                    trainer.scaler.update()\n",
        "                    trainer.scheduler.step()\n",
        "                \n",
        "                epoch_loss += loss.item() * grad_accum\n",
        "                batch_count += 1\n",
        "        else:\n",
        "            logits, loss = trainer.model.model(x_t, y_t)\n",
        "            \n",
        "            if loss is not None:\n",
        "                loss = loss / grad_accum\n",
        "                loss.backward()\n",
        "                \n",
        "                if (batch_idx + 1) % grad_accum == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(\n",
        "                        trainer.model.model.parameters(), \n",
        "                        CONFIG[\"max_grad_norm\"]\n",
        "                    )\n",
        "                    trainer.optimizer.step()\n",
        "                    trainer.scheduler.step()\n",
        "                \n",
        "                epoch_loss += loss.item() * grad_accum\n",
        "                batch_count += 1\n",
        "        \n",
        "        pbar.update(1)\n",
        "        current_lr = trainer.scheduler.get_last_lr()[0] if trainer.scheduler else CONFIG[\"learning_rate\"]\n",
        "        pbar.set_postfix({\n",
        "            \"loss\": f\"{(loss.item() * grad_accum if loss else 0):.4f}\",\n",
        "            \"epoch\": epoch+1,\n",
        "            \"lr\": f\"{current_lr:.6f}\"\n",
        "        })\n",
        "    \n",
        "    avg_loss = epoch_loss / max(batch_count, 1)\n",
        "    epoch_losses.append(avg_loss)\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    batches_done = (epoch + 1) * config.max_batches\n",
        "    eta = (elapsed / batches_done) * (total_batches - batches_done) if batches_done > 0 else 0\n",
        "    \n",
        "    print(f\"\\nüìä Epoch {epoch+1}/{CONFIG['epochs']} | Loss: {avg_loss:.4f} | LR: {current_lr:.6f} | ETA: {eta/60:.1f}min\")\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "print(f\"   Total time: {(time.time()-start_time)/60:.1f} minutes\")\n",
        "print(f\"   Final loss: {epoch_losses[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Save trained model\n",
        "output_dir = Path(\"models/sloughgpt\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model_path = output_dir / \"sloughgpt_trained.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': trainer.model.model.state_dict(),\n",
        "    'config': CONFIG,\n",
        "    'training_config': config.to_dict(),\n",
        "    'data_path': str(data_path),\n",
        "}, model_path)\n",
        "\n",
        "print(f\"üíæ Model saved to: {model_path}\")\n",
        "\n",
        "# Download to local machine\n",
        "from google.colab import files\n",
        "files.download(str(model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text with trained model\n",
        "print(\"=\"*60)\n",
        "print(\"üéØ TEXT GENERATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "PROMPT = \"The king\"\n",
        "MAX_TOKENS = 200\n",
        "TEMPERATURE = 0.8\n",
        "\n",
        "print(f\"Prompt: '{PROMPT}'\")\n",
        "\n",
        "trainer.model.model.eval()\n",
        "\n",
        "# Simple tokenizer\n",
        "chars = sorted(set(trainer.data_loader.texts[0]))\n",
        "stoi = {c: i for i, c in enumerate(chars)}\n",
        "itos = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "idx = [stoi.get(c, 0) for c in PROMPT]\n",
        "idx = torch.tensor([idx], dtype=torch.long).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(MAX_TOKENS):\n",
        "        idx_cond = idx[:, -128:]\n",
        "        \n",
        "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "            logits, _ = trainer.model.model(idx_cond)\n",
        "        \n",
        "        logits = logits[:, -1, :] / TEMPERATURE\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, idx_next], dim=1)\n",
        "\n",
        "generated = ''.join([itos.get(i, '') for i in idx[0].tolist()])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìù GENERATED TEXT:\")\n",
        "print(\"=\"*60)\n",
        "print(generated)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Personality Training (NEW!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train personality using neural personality module\n",
        "from domains.neural_personality import NeuralPersonality\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üé≠ PERSONALITY TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create neural personality\n",
        "np_model = NeuralPersonality(vocab_size=100)\n",
        "\n",
        "# Training data for personality\n",
        "personality_data = [\n",
        "    {\"text\": \"Hey friend! How wonderful to see you today!\", \n",
        "     \"traits\": {\"warmth\": 0.9, \"formality\": -0.8, \"humor\": 0.7}},\n",
        "    {\"text\": \"I would like to submit this proposal formally.\",\n",
        "     \"traits\": {\"warmth\": 0.0, \"formality\": 0.9, \"confidence\": 0.7}},\n",
        "    {\"text\": \"Imagine a world of endless possibilities!\",\n",
        "     \"traits\": {\"warmth\": 0.7, \"creativity\": 0.9, \"humor\": 0.5}},\n",
        "    {\"text\": \"I understand how you feel. Tell me more.\",\n",
        "     \"traits\": {\"empathy\": 0.9, \"patience\": 0.8, \"warmth\": 0.7}},\n",
        "]\n",
        "\n",
        "# Train personality\n",
        "np_model.train(personality_data, epochs=100, lr=0.1)\n",
        "\n",
        "# Test personality\n",
        "test_text = \"Hello! Great to meet you!\"\n",
        "traits = np_model.predict_traits(test_text)\n",
        "print(f\"\\nüìä Personality traits for: '{test_text}'\")\n",
        "for trait, value in traits.items():\n",
        "    if abs(value) > 0.1:\n",
        "        print(f\"   {trait}: {value:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. .sou Model Format (NEW!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create .sou model configuration (like Ollama Modelfile)\n",
        "from domains.inference import SouModelFile, SouParser, GenerationParameters, PersonalityConfig\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üì¶ .sou MODEL FORMAT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create .sou config\n",
        "sou_config = SouModelFile(\n",
        "    from_model=\"llama3.2\",  # or path to your trained model\n",
        "    parameters=GenerationParameters(\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        max_tokens=2048,\n",
        "    ),\n",
        "    system=\"You are a helpful AI assistant.\",\n",
        "    personality=PersonalityConfig(\n",
        "        warmth=0.8,\n",
        "        creativity=0.6,\n",
        "        empathy=0.7,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Generate .sou file content\n",
        "sou_content = f\"\"\"FROM {sou_config.from_model}\n",
        "PARAMETER temperature {sou_config.parameters.temperature}\n",
        "PARAMETER top_p {sou_config.parameters.top_p}\n",
        "PARAMETER max_tokens {sou_config.parameters.max_tokens}\n",
        "\n",
        "PERSONALITY\n",
        "    warmth {sou_config.personality.warmth}\n",
        "    creativity {sou_config.personality.creativity}\n",
        "    empathy {sou_config.personality.empathy}\n",
        "    END\n",
        "\n",
        "SYSTEM {sou_config.system}\n",
        "\n",
        "METADATA author \"SloughGPT User\"\n",
        "METADATA version \"1.0.0\"\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìÑ Generated .sou file:\")\n",
        "print(sou_content)\n",
        "\n",
        "# Save .sou file\n",
        "sou_path = Path(\"models/sloughgpt/model.sou\")\n",
        "sou_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "sou_path.write_text(sou_content)\n",
        "print(f\"‚úÖ .sou file saved to: {sou_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Personality Metrics Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze personality metrics\n",
        "from domains.ai_personality_metrics import PersonalityMetrics, TextAnalyzer\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìä PERSONALITY METRICS ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_texts = [\n",
        "    \"Hey there! How are you doing today? üòä\",\n",
        "    \"I understand your concern. Please allow me to assist you with a comprehensive solution.\",\n",
        "    \"IDK lol idc TBH üòÖ\",\n",
        "    \"This is a neutral statement about the weather.\",\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    metrics = PersonalityMetrics.compute_all_metrics(text)\n",
        "    print(f\"\\nüìù '{text[:40]}...'\")\n",
        "    print(f\"   Friendliness: {metrics['friendliness']:.2f}\")\n",
        "    print(f\"   Helpfulness: {metrics['helpfulness']:.2f}\")\n",
        "    print(f\"   Creativity:   {metrics['creativity']:.2f}\")\n",
        "    print(f\"   Formality:   {metrics['formality']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "You've trained a model with:\n",
        "‚úÖ **Industry-Standard Training:**\n",
        "   - LR Schedulers: Cosine annealing with warmup\n",
        "   - Mixed Precision: BF16/FP16\n",
        "   - Gradient Accumulation\n",
        "   - Gradient Clipping\n",
        "\n",
        "‚úÖ **Personality System:**\n",
        "   - Neural personality training\n",
        "   - Real computational metrics\n",
        "   - Configurable personality traits\n",
        "\n",
        "‚úÖ **.sou Format:**\n",
        "   - Ollama-inspired model configuration\n",
        "   - Personality embeddings\n",
        "   - Quantization support"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
