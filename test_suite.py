#!/usr/bin/env python3
"""
SloughGPT Comprehensive Test Suite
Unit, integration, performance, and security tests
"""

import sys
import os
import time
import asyncio
from pathlib import Path

# Add sloughgpt to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sloughgpt.core.testing import (
    run_all_tests, TestConfiguration, unit_test, integration_test, 
    performance_test, security_test
)

def main():
    """Run comprehensive test suite"""
    print("ğŸ§ª SloughGPT Comprehensive Test Suite")
    print("=" * 60)
    
    # Create test configuration
    config = TestConfiguration(
        test_data_dir="test_data",
        output_dir="test_results",
        parallel_workers=2,
        timeout_seconds=30,
        coverage_threshold=80.0,
        performance_baseline_ms=50.0,
        enable_slow_tests=True,
        enable_integration_tests=True,
        enable_security_tests=True
    )
    
    print("\nğŸ”§ Test Configuration:")
    print(f"   ğŸ“ Test Data Directory: {config.test_data_dir}")
    print(f"   ğŸ“Š Output Directory: {config.output_dir}")
    print(f"   ğŸ‘¥ Parallel Workers: {config.parallel_workers}")
    print(f"   â±ï¸ Timeout: {config.timeout_seconds}s")
    print(f"   ğŸ“ˆ Coverage Threshold: {config.coverage_threshold}%")
    print(f"   âš¡ Performance Baseline: {config.performance_baseline_ms}ms")
    
    print(f"\nğŸš€ Running Tests...")
    
    # Create test directories
    os.makedirs(config.test_data_dir, exist_ok=True)
    os.makedirs(config.output_dir, exist_ok=True)
    
    # Run comprehensive test suite
    start_time = time.time()
    results = run_all_tests(config)
    end_time = time.time()
    
    # Generate report
    print(f"\nğŸ“Š Test Results (completed in {end_time - start_time:.2f}s):")
    
    for suite_name, suite_data in results["suites"].items():
        print(f"\nğŸ“‹ {suite_name.title()} Suite:")
        print(f"   Status: {suite_data['status']}")
        print(f"   Pass Rate: {suite_data['summary']['pass_rate']:.1f}%")
        print(f"   Total Duration: {suite_data['total_duration']:.2f}s")
        print(f"   Tests: {suite_data['summary']['total_tests']}")
        print(f"   Passed: {suite_data['summary']['passed']}")
        print(f"   Failed: {suite_data['summary']['failed']}")
        
        # Show key results
        if suite_data["results"]:
            print(f"\n   ğŸ” Key Results:")
            for i, result in enumerate(suite_data["results"][:5]):  # Show first 5
                status_icon = "âœ…" if result.status.value == "passed" else "âŒ"
                print(f"      {status_icon} {result.test_name}: {result.status.value} ({result.duration_ms:.1f}ms)")
    
    # Generate summary
    print(f"\nğŸ“ˆ Overall Summary:")
    summary = results["summary"]
    print(f"   ğŸ§ª Total Tests: {summary['total_tests']}")
    print(f"   âœ… Passed: {summary['passed']}")
    print(f"   âŒ Failed: {summary['failed']}")
    print(f"   âš ï¸ Errors: {summary['errors']}")
    print(f"   ğŸ“Š Pass Rate: {summary['pass_rate']:.1f}%")
    print(f"   ğŸ“ˆ Test Suites: {summary['total_suites']}")
    
    # Performance benchmarks
    if "performance_tests" in results:
        perf_suite = results["performance_tests"]
        print(f"\nâš¡ Performance Benchmarks:")
        
        for result in perf_suite["results"]:
            if result.metadata:
                print(f"   â€¢ {result.test_name}:")
                if "avg_ms" in result.metadata:
                    print(f"     Average: {result.metadata['avg_ms']:.2f}ms")
                if "baseline_ratio" in result.metadata:
                    print(f"     Baseline Ratio: {result.metadata['baseline_ratio']:.2f}x")
    
    # Security test summary
    if "security_tests" in results:
        sec_suite = results["security_tests"]
        print(f"\nğŸ›¡ï¸ Security Test Summary:")
        
        malicious_tests = [r for r in sec_suite["results"] if "malicious" in r.test_name.lower()]
        blocked_tests = [r for r in sec_suite["results"] if r.status.value == "passed" and "malicious" in r.test_name.lower()]
        
        passed_tests = [r for r in sec_suite["results"] if r.status.value == "passed"]
        print(f"   ğŸ›‘ï¸ Malicious Tests Blocked: {len(blocked_tests)}/{len(malicious_tests)}")
        print(f"   âœ… Security Tests Passed: {len(passed_tests)}/{len(sec_suite['results'])}")
    
    # Save detailed report
    output_file = Path(config.output_dir) / "test_report.json"
    
    import json
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nğŸ“„ Detailed report saved to: {output_file}")
    
    # Show health status
    print(f"\nğŸ¥ Test Infrastructure Health:")
    
    # Check if tests passed thresholds
    overall_health = "ğŸŸ¢ Healthy"
    if summary['pass_rate'] < config.coverage_threshold:
        overall_health = "ğŸŸ¡ Needs Improvement"
    
    if summary['errors'] > 0:
        overall_health = "ğŸ”´ Has Issues"
    
    print(f"   Status: {overall_health}")
    print(f"   Pass Rate: {summary['pass_rate']:.1f}% (Target: {config.coverage_threshold}%)")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ Comprehensive Test Suite Completed!")
    print("\nğŸš€ Test Infrastructure Features:")
    print("   âœ… Unit testing framework")
    print("   âœ… Integration testing")
    print("   âœ… Performance benchmarking")
    print("   âœ… Security testing")
    print("   âœ… Mock external services")
    print("   âœ… Parallel execution")
    print("   âœ… Comprehensive reporting")
    print("   âœ… Configurable test suites")
    print("   âœ… Performance monitoring")
    print("   âœ… Extensible architecture")
    print("\nğŸ›¡ï¸ Production-Ready Testing Framework!")
    print("   â€¢ Automated test discovery")
    print("   â€¢ Parallel test execution")
    print("   â€¢ Performance regression detection")
    print("   â€¢ Security vulnerability scanning")
    print("   â€¢ Comprehensive reporting")
    print("   â€¢ CI/CD integration ready")
    print("   â€¢ Coverage measurement")
    print("   â€¢ Mock service simulation")
    
    # Cleanup
    if os.path.exists(config.test_data_dir):
        import shutil
        shutil.rmtree(config.test_data_dir)
    
    print(f"\nğŸ§¹ Cleanup completed")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Test suite interrupted by user")
    except Exception as e:
        print(f"\nâŒ Test suite failed: {e}")
        import traceback
        traceback.print_exc()