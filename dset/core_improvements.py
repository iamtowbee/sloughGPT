#!/usr/bin/env python3
"""dset ‚Äì Core Logic Improvements

Focus on core functionality: fetch, validate, organize datasets efficiently.
"""

import argparse
import json
import sys
import tempfile
import shutil
from pathlib import Path
from typing import Optional, List, Dict, Set
import concurrent.futures
import time

from .repo_obtainer import (
    run_lmtrain_web_search,
    run_lmtrain_github_search,
    fetch_to_corpus,
)
from .stats import DatasetStats


class DatasetOptimizer:
    """Optimizes datasets for training efficiency"""
    
    def __init__(self):
        self.deduplication_cache = {}
        self.quality_filters = set()
    
    def deduplicate(self, examples: List[Dict]) -> List[Dict]:
        """Remove duplicate examples based on content similarity"""
        unique_examples = []
        seen_content = set()
        
        for example in examples:
            content = example.get("content", "")
            content_hash = hash(content) if content else ""
            
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_examples.append(example)
        
        removed = len(examples) - len(unique_examples)
        if removed > 0:
            print(f"üóëÔ∏è  Removed {removed} duplicate examples")
        
        return unique_examples
    
    def filter_by_quality(self, examples: List[Dict]) -> List[Dict]:
        """Filter out low-quality examples"""
        filtered = []
        
        for example in examples:
            content = example.get("content", "")
            
            # Quality heuristics
            if len(content) < 50:  # Too short
                continue
            if content.count('\n') > 20:  # Too many lines (likely noise)
                continue
            if len(set(content)) < 10:  # Low diversity
                continue
            
            filtered.append(example)
        
        removed = len(examples) - len(filtered)
        if removed > 0:
            print(f"üöÆ Filtered {removed} low-quality examples")
        
        return filtered
    
    def balance_by_language(self, examples: List[Dict]) -> List[Dict]:
        """Balance examples across programming languages if possible"""
        # Count examples by language
        lang_counts = {}
        for example in examples:
            content = example.get("content", "")
            lang = self.detect_language(content)
            lang_counts[lang] = lang_counts.get(lang, 0) + 1
        
        # Balance by undersampling
        max_count = max(lang_counts.values()) if lang_counts else 0
        balanced = []
        
        for example in examples:
            content = example.get("content", "")
            lang = self.detect_language(content)
            if lang_counts[lang] > max_count * 0.7:  # Don't over-sample
                continue
            balanced.append(example)
        
        print(f"‚öñÔ∏è Balanced dataset across {len(lang_counts)} languages")
        return balanced
    
    def detect_language(self, content: str) -> str:
        """Simple language detection from content"""
        content_lower = content.lower()
        
        if "python" in content_lower or "import numpy" in content_lower:
            return "python"
        elif "javascript" in content_lower or "react" in content_lower:
            return "javascript"
        elif "typescript" in content_lower:
            return "typescript"
        elif "sql" in content_lower:
            return "sql"
        else:
            return "unknown"


class ParallelFetcher:
    """Parallel fetching capabilities"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
    
    def fetch_multiple(self, queries: List[str], source: str, **kwargs) -> List[Dict]:
        """Fetch multiple datasets in parallel"""
        results = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all fetch tasks
            future_to_query = {}
            for query in queries:
                if source == "web":
                    future = executor.submit(run_lmtrain_web_search, query=query, **kwargs)
                else:
                    future = executor.submit(run_lmtrain_github_search, query=query, **kwargs)
                future_to_query[future] = query
            
            # Collect results as they complete
            for future in concurrent.futures.as_completed(future_to_query):
                query = future_to_query[future]
                try:
                    success = future.result()
                    if success:
                        results.append({"query": query, "status": "success"})
                        print(f"‚úÖ {query}: Success")
                    else:
                        results.append({"query": query, "status": "failed"})
                        print(f"‚ùå {query}: Failed")
                except Exception as e:
                    results.append({"query": query, "status": "error", "error": str(e)})
                    print(f"‚ö†Ô∏è {query}: Error - {e}")
        
        return results


def improved_main() -> None:
    """Core improvements focus"""
    parser = argparse.ArgumentParser(
        description="dset ‚Äì core logic improvements",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Core Improvements:
  ‚Ä¢ Parallel fetching for multiple queries
  ‚Ä¢ Dataset optimization (deduplication, quality filtering, balancing)
  ‚Ä¢ Better error handling and recovery
  ‚Ä¢ Efficient memory usage
        """
    )
    
    # Core arguments
    parser.add_argument(
        "--source",
        choices=["web", "github"],
        default="web",
        help="Source: web search or GitHub repositories"
    )
    parser.add_argument("--query", help="Search query (or use --queries for multiple)")
    parser.add_argument("--queries", nargs='+', help="Multiple search queries")
    parser.add_argument("--dataset", required=True, help="Dataset name (output: runs/<name>.jsonl)")
    
    # Quality and optimization
    parser.add_argument("--deduplicate", action="store_true", help="Remove duplicate examples")
    parser.add_argument("--quality-filter", action="store_true", help="Filter low-quality examples")
    parser.add_argument("--balance", action="store_true", help="Balance across languages")
    parser.add_argument("--parallel", type=int, default=1, help="Parallel fetch workers")
    
    # Performance options
    parser.add_argument("--max-examples", type=int, default=100, help="Max examples per query")
    parser.add_argument("--language", help="Filter by language")
    parser.add_argument("--format", default="instruction", choices=["instruction", "completion", "chat"])
    parser.add_argument("--output", help="Custom output path")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    
    args = parser.parse_args()
    
    # Handle single vs multiple queries
    if args.query:
        queries = [args.query]
    elif args.queries:
        queries = args.queries
    else:
        print("‚ùå Must provide --query or --queries")
        sys.exit(1)
    
    # Initialize components
    stats = DatasetStats()
    optimizer = DatasetOptimizer()
    fetcher = ParallelFetcher(max_workers=args.parallel)
    
    print(f"üöÄ Starting dataset collection for {len(queries)} quer{'ies' if len(queries) > 1 else 'y'}")
    stats.start(args.source, ", ".join(queries))
    
    # Setup temporary directory
    temp_dir = Path(tempfile.mkdtemp(prefix="dset_improved_"))
    
    try:
        # Fetch data
        all_examples = []
        lmtrain_files = []
        
        if len(queries) == 1:
            # Single query - standard path
            lmtrain_out = temp_dir / "lmtrain_output.jsonl"
            success = (run_lmtrain_web_search if args.source == "web" else run_lmtrain_github_search)(
                query=queries[0],
                output_path=str(lmtrain_out),
                max_examples=args.max_examples,
                language=args.language,
                format_type=args.format,
            )
            
            if success:
                examples = json.loads(lmtrain_out.read_text())
                all_examples.extend(examples if isinstance(examples, list) else [examples])
                lmtrain_files.append(lmtrain_out)
        else:
            # Multiple queries - parallel fetching
            fetch_kwargs = {
                "max_examples": args.max_examples,
                "language": args.language,
                "format_type": args.format,
                "output_path": str(temp_dir / "parallel_{i}.jsonl")  # Will be created per query
            }
            
            # We'll implement a simpler version for now
            for i, query in enumerate(queries):
                lmtrain_out = temp_dir / f"parallel_{i}.jsonl"
                success = (run_lmtrain_web_search if args.source == "web" else run_lmtrain_github_search)(
                    query=query,
                    output_path=str(lmtrain_out),
                    max_examples=args.max_examples,
                    language=args.language,
                    format_type=args.format,
                )
                
                if success:
                    examples = json.loads(lmtrain_out.read_text())
                    all_examples.extend(examples if isinstance(examples, list) else [examples])
                    lmtrain_files.append(lmtrain_out)
        
        if not all_examples:
            print("‚ùå No examples fetched")
            return
        
        # Apply optimizations
        print(f"\nüîß Starting with {len(all_examples)} examples")
        
        if args.deduplicate:
            all_examples = optimizer.deduplicate(all_examples)
        
        if args.quality_filter:
            all_examples = optimizer.filter_by_quality(all_examples)
        
        if args.balance:
            all_examples = optimizer.balance_by_language(all_examples)
        
        # Write final corpus
        out_path = Path(args.output) if args.output else Path("runs") / f"{args.dataset}.jsonl"
        out_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Convert and write
        final_lmtrain_path = temp_dir / "final_lmtrain.jsonl"
        with open(final_lmtrain_path, "w") as f:
            for example in all_examples:
                f.write(json.dumps(example) + "\n")
        
        converted = fetch_to_corpus(str(final_lmtrain_path), str(out_path))
        
        # Complete stats
        stats.finish(converted, out_path)
        stats.print_summary()
        
        if args.verbose:
            print(f"\nüìà Dataset Quality Metrics:")
            print(f"   Total processed: {len(all_examples)}")
            print(f"   Final examples: {converted}")
            print(f"   Quality filters: {args.deduplicate}, {args.quality_filter}, {args.balance}")
    
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Operation cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)
    finally:
        # Cleanup
        shutil.rmtree(temp_dir, ignore_errors=True)
        print("üßπ Cleaned up temporary files")


if __name__ == "__main__":
    improved_main()